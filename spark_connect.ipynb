{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fec6d3c2-428f-468a-a3e8-9e059c132dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 概述\n",
    "EMR Serverless Spark是一款云原生，专为大规模数据处理和分析而设计的全托管Serverless产品。它为企业提供了一站式的数据平台服务，包括Spark任务调试、调度和运维等，极大地简化了数据处理的全生命周期工作流程。\n",
    "您可以在DSW中，利用Serverless Spark提供的Livy API，远程连接Serverless Spark集群，并提交Spark任务到服务端执行。关于Livy API，请参见[REST API](https://livy.incubator.apache.org/docs/latest/rest-api.html)。\n",
    "![image.png](_html/1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a828b27d-8b82-46cc-be93-ae5866be1c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 使用限制\n",
    "- DSW实例作为客户端，对资源规格没有特殊要求，推荐规格仅为建议，用户可按需选择需要的CPU/GPU规格。\n",
    "- 推荐使用官方镜像 pytorch-develop:2.1-cpu-py311-ubuntu22.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd328d88-341b-4c87-a655-0b80e6536326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 前提条件\n",
    "1. 开通并创建[EMR Serverless Spark工作空间](https://help.aliyun.com/zh/emr/emr-serverless-spark/getting-started/create-a-workspace)\n",
    "2. 在Spark控制台[创建Gateway及访问Token](https://help.aliyun.com/zh/emr/emr-serverless-spark/use-cases/use-the-sparkmagic-plugin-of-jupyter-notebook-to-interact-with-serverless-spark)，**后续的连接步骤需要Gateway的Endpoint及Token信息**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66eb7749-13bd-4778-96ed-54efd976296d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 使用步骤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b303c450-3ba5-48b8-a106-0edaf3d76976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 步骤一：安装sparkmagic插件\n",
    "执行以下命令，安装sparkmagic插件。Sparkmagic插件的更多详细信息和高级配置选项，请参见[sparkmagic](https://github.com/jupyter-incubator/sparkmagic)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed70847-18ae-473b-9131-6447621ab27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install library\n",
    "!pip install sparkmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "830d8622-b04d-4da5-ab35-8c3483ed084c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 步骤二：配置与启动Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "963cf999-6f16-4c95-90e2-b44263729d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-08-15T10:44:14.161029Z",
     "iopub.status.busy": "2024-08-15T10:44:14.160672Z",
     "iopub.status.idle": "2024-08-15T10:44:15.146324Z",
     "shell.execute_reply": "2024-08-15T10:44:15.145645Z",
     "shell.execute_reply.started": "2024-08-15T10:44:14.161006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 载入sparkmagic插件\n",
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c9e3af4-8652-41e8-a161-a8d7b8faa3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1（推荐）使用sparkmagic的管理界面进行session管理\n",
    "详细配置及参数文档参考[通过Jupyter Notebook的sparkmagic插件与Serverless Spark进行交互](https://help.aliyun.com/zh/emr/emr-serverless-spark/use-cases/use-the-sparkmagic-plugin-of-jupyter-notebook-to-interact-with-serverless-spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeeac841-07d3-4fd3-a533-4d4a015fbf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-08-19T06:39:15.752532Z",
     "iopub.status.busy": "2024-08-19T06:39:15.751786Z",
     "iopub.status.idle": "2024-08-19T06:39:15.764826Z",
     "shell.execute_reply": "2024-08-19T06:39:15.763831Z",
     "shell.execute_reply.started": "2024-08-19T06:39:15.752506Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 打开sparkmagic配置UI，在界面上操作之前，请先执行下一个cell调大session创建超时时间，再继续创建Endpoint及Session\n",
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47eac28e-dbce-4220-9c02-6c75a7ab9482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 注意，在启动session前，需要调大sparkmagic插件的启动session超时时间，否则可能会出现无法启动session的情况。\n",
    "import sparkmagic.utils.configuration as conf\n",
    "conf.override(\"livy_session_startup_timeout_seconds\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0148d4b0-716c-4157-bef2-54eb2f018844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 在管理界面中配置Spark Session\n",
    "在上一步中打开的插件管理界面中，按如下操作步骤（a~c）进行配置并创建Session，推荐在红框区域进行配置操作，非红框区域用作配置预览：\n",
    "  ![image.png](_html/2.jpg)\n",
    "a. 管理Endpoint配置，在【Add Endpoint】页签填写相关参数，填写完成后点击配置页面最右侧**Add endpoint**按钮\n",
    "  ![image.png](_html/3.jpg)\n",
    "  【参数说明】其中Address和Password需要在EMR Spark控制台-工作空间-管理员配置-Compute-Gateway中获取，获取方法[参考文档](https://help.aliyun.com/zh/emr/emr-serverless-spark/use-cases/use-the-sparkmagic-plugin-of-jupyter-notebook-to-interact-with-serverless-spark#b742f2a133x2y)\n",
    "|参数|说明|\n",
    "|-|-|\n",
    "|Auth type|选择Basic_Access。|\n",
    "|Address|填写格式为https://<Gateway的Endpoint信息>。|\n",
    "|Username|使用默认值即可。|\n",
    "|Password|在EMR Spark控制台-工作空间-管理员配置-Compute-Gateway-Token管理中获取的Token。|\n",
    "\n",
    "b. 切换到【Create Session】页签，选择刚创建好的Endpoint，自定义一个session名称，语言选择Python，其他参数保持默认，点击配置页面最右侧**Create Session**按钮\n",
    "  ![image.png](_html/4.jpg)\n",
    "c. 这时DSW实例的kernel会进入busy状态，创建session大约需要1~5分钟，创建完成后，可以在【Manage Sessions】页签看到刚创建好的session。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d4567c2-654c-4917-8c48-af08ea0d0afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2（可选）不使用UI，直接使用命令创建spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc02f555-83ca-436c-ac33-8ed722089d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-08-15T10:46:45.171089Z",
     "iopub.status.busy": "2024-08-15T10:46:45.170842Z",
     "iopub.status.idle": "2024-08-15T10:46:45.174560Z",
     "shell.execute_reply": "2024-08-15T10:46:45.173988Z",
     "shell.execute_reply.started": "2024-08-15T10:46:45.171069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 同样在新建session前进行超时配置\n",
    "import sparkmagic.utils.configuration as conf\n",
    "conf.override(\"livy_session_startup_timeout_seconds\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96b59ff1-2a27-4438-a3ea-9db7027f1f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 如不习惯使用管理界面操作，上一小节中的配置可以用如下命令替换，执行后新建一个spark session\n",
    "# Endpoint和Token需要在EMR Spark控制台-工作空间-管理员配置-Compute-Gateway中获取\n",
    "# 例如：%spark add -s customsession -l python -u http://emr-spark-livy-gateway-cn-hangzhou.data.aliyun.com/api/v1/workspace/w-f***********/livycompute/lc-fim********* -a username -p u3gg*********\n",
    "%spark add -s 自定义session名称 -l python -u https://填写Gateway的Endpoint -a username -p 填写Gateway的Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14069440-721a-44ed-9228-d80486cb9762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 更多%spark命令参数说明可以执行%spark?来查看\n",
    "%spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a27be8-2a68-4e81-a1c4-619208e10554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# 如果多个session同时在运行，在执行spark migic时，可以通过这种方式做session指定 %%spark --session 运行中的sessionname\n",
    "\n",
    "#查看Spark版本\n",
    "print(\"Spark Version:\", sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ac9c2de-7f0c-456a-89da-74709bbbba49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 步骤三：使用PySpark开发并提交训练任务\n",
    "以训练一个线性回归模型为例，我们通过spark magic提交模型训练及预测任务到Spark集群计算，并获取打印结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ae6b761-8e89-4020-86e5-80b9b671c36b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-08-19T07:17:30.313594Z",
     "iopub.status.busy": "2024-08-19T07:17:30.312890Z",
     "iopub.status.idle": "2024-08-19T07:17:46.069813Z",
     "shell.execute_reply": "2024-08-19T07:17:46.068689Z",
     "shell.execute_reply.started": "2024-08-19T07:17:30.313565Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 构造一个简单的DataFrame作为示例数据\n",
    "data = [(1, 1), (2, 3), (3, 5), (4, 7), (5, 9)]  # 特征和标签数据\n",
    "columns = [\"feature\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 使用VectorAssembler将特征转换为向量，因为许多ML算法要求输入数据为向量形式\n",
    "assembler = VectorAssembler(inputCols=[\"feature\"], outputCol=\"features_vector\")\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# 选择LinearRegression模型进行训练\n",
    "lr = LinearRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "\n",
    "# 划分训练集（这里为了简化直接使用全部数据训练，实际应用中应划分为训练集和测试集）\n",
    "train_data = df_transformed\n",
    "\n",
    "# 训练模型\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# 打印模型参数\n",
    "print(\"模型系数:\", model.coefficients)\n",
    "print(\"模型截距:\", model.intercept)\n",
    "\n",
    "# 使用模型进行预测（以第一行数据为例）\n",
    "input_data = [[1]]  # 特征数据\n",
    "input_df = spark.createDataFrame(input_data, [\"feature\"])\n",
    "input_df = assembler.transform(input_df)\n",
    "prediction = model.transform(input_df)\n",
    "print(\"预测结果:\", prediction.select(\"prediction\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bf2c598-0d9a-499c-9e7a-ab0c534758e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 步骤四：释放Session资源\n",
    "创建的Session会在闲置达到两小时后自动终止，确保资源的及时回收。此外，您也可以手动单击sparkmagic插件界面上的Delete来提前结束并释放会话资源。"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_connect",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "share": {
   "datetime": "2024-07-25T14:16:26.642Z",
   "image": {
    "name": "pytorch-develop:2.1-cpu-py311-ubuntu22.04",
    "url": "dsw-registry-vpc.cn-hangzhou.cr.aliyuncs.com/pai/pytorch:2.1-cpu-py311-ubuntu22.04"
   },
   "instance": "dsw-60c1b34a514b3415",
   "spec": {
    "id": "ecs.g6.xlarge",
    "type": "CPU"
   },
   "uid": "1157703270994901"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
